{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93313290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCRIPT: SAIHEBRO.py\n",
    "# AUTHOR: Henrik Schmidt\n",
    "# DATE: 13.03.2022\n",
    "#==================================================================================================================\n",
    "# This script contains functions to clean-up SAIHEBRO meteorogical parameters csv-files from redundant whitespaces \n",
    "# and merge different parameter-files stationwise. For this purpose csv-files and one or more text-files \n",
    "# are needed. The text-files should contain the station-numbers where each station should occupie an extra line.\n",
    "# E.g.:\n",
    "# P080\n",
    "# P081\n",
    "# ...\n",
    "#==================================================================================================================\n",
    "import matplotlib as plt\n",
    "import pandas as pd\n",
    "import os, re, glob\n",
    "import numpy as np\n",
    "\n",
    "#==================================================================================================================\n",
    "# Function to add a sign in as first element in a file\n",
    "def add_firstsign(path,sign='#'):\n",
    "    os.chdir(path) # changing active path\n",
    "    file_list = glob.glob('*.{}'.format('csv')) # list all csv files in the specified path\n",
    "    for file in file_list: # iterate through the filelist\n",
    "        file_read = open(file,'r') # read the lines out of the file\n",
    "        lines = file_read.readlines()\n",
    "        first_line = sign + lines[0] # add the specified sign, standard #\n",
    "        file_read.close() \n",
    "        file_write = open(file,'w') # write the new first line and all the other lines (file will be empty otherwise)\n",
    "        for i in range(len(lines)):\n",
    "            if i == 0:\n",
    "                file_write.write(first_line) # write the first line in the first place\n",
    "            else:\n",
    "                file_write.write(lines[i]) # write all the other lines\n",
    "        file_write.close()\n",
    "    print('Added # in first row...') # Progress\n",
    "#==================================================================================================================\n",
    "# Function to extract all the station numbers as an array\n",
    "def array_stations(path):\n",
    "    file = open(path,'r') # Open the file\n",
    "    array_stations = file.readlines() # Read out each line and place them in an array\n",
    "    # Check for ',' in the array (E.g.: P080,TEMPE OR P080)\n",
    "    test = re.search(',', array_stations[0])\n",
    "    if test != None: # E.g.: P080,TEMPE in text-file\n",
    "        for value, line in enumerate(array_stations,0):\n",
    "            array_stations[value] = line.split(',') # Split the line by ','\n",
    "            array_stations[value] = array_stations[value][0] # Just keep the first element\n",
    "            # E.g.: Returns only P080\n",
    "    else: # E.g.: P080 in text-file\n",
    "        array_stations = [line.strip() for line in file.readlines()] # Keep the line and just erase whitespaces\n",
    "    file = None # Close the file, so no read/write conflict occurs\n",
    "    \n",
    "    return array_stations # Return the array\n",
    "#==================================================================================================================\n",
    "# Function to clean the saihebro csv-files from redundant whitespaces\n",
    "# BEFORE CLEANING E.g.: date     ;  measurment_a         ;  measurement_b        ;....\n",
    "# AFTER CLEANING E.g.: date; measurment_a; measurement_b; ....\n",
    "def clean_saihebro(file):\n",
    "    with open(file, 'r+') as f:\n",
    "        csv = f.read().replace('  ', '').replace(' ;', ';')\n",
    "        f.seek(0)\n",
    "        f.write(csv)\n",
    "        f.truncate()\n",
    "#==================================================================================================================\n",
    "# Function to combine the parameters stationwise\n",
    "# Always check the column headers, mostlikely the clean function will skip some whitespaces. CTRL + F is your friend.\n",
    "def combine(array_stations,array_parameter,path,start_date=\"1998-01-01\",end_date=\"2021-12-31\"):\n",
    "    # Generate timespan as an array\n",
    "    times = pd.date_range(start=start_date,end=end_date).to_list()\n",
    "    # Iterate through the text-file stations\n",
    "    for station in array_stations:\n",
    "        # Initiate parameter array, all used parameters for the station will be stored here (used in file-naming)\n",
    "        parameter_array = [] \n",
    "        # Initiate Pandas dataframe with timespan as first index column (key for later join operations)\n",
    "        df_main = pd.DataFrame(data=times,columns=['date'])\n",
    "        for file in array_parameter:\n",
    "            if file[1]==station:\n",
    "                clean_saihebro(file[0]) # Call clean_saihebro function\n",
    "                df = pd.read_csv(file[0], encoding = 'latin', sep = ';') # Read csv-file as dataframe\n",
    "                # ------------------------------------------------------------------------------------------------\n",
    "                # Dataframe cleaning via parameter:\n",
    "                if file[2]=='TEMPE':\n",
    "                    df.drop(['Fecha máximo','Fecha mínimo'], axis=1,inplace=True) # Drop redundant columns\n",
    "                    df.rename(columns={\"Máximo\": \"T_max\",\"Mínimo\": \"T_min\",\"Media\": \"T_mean\", \"fecha \": \"date\"}, inplace=True)\n",
    "                    df['date'] = pd.to_datetime(df['date']) # Change date datatype to datetime\n",
    "                    df['T_max'] = df['T_max'].str.replace(',', '.') # Change punctuation to international\n",
    "                    df['T_min'] = df['T_min'].str.replace(',', '.') # Change punctuation to international\n",
    "                    df['T_mean'] = df['T_mean'].str.replace(',', '.') # Change punctuation to international\n",
    "                    df['T_max'] = df['T_max'].astype(float) # Change datatype to float\n",
    "                    df['T_min'] = df['T_min'].astype(float) # Change datatype to float\n",
    "                    df['T_mean'] = df['T_mean'].astype(float) # Change datatype to float\n",
    "                # ------------------------------------------------------------------------------------------------\n",
    "                if file[2]=='PACUM':\n",
    "                    df.drop(['Fecha acumulado','Máximo','Fecha máximo'], axis=1,inplace=True) # Drop redundant columns\n",
    "                    df.rename(columns={\"Acumulado\": \"precipitation\", \"fecha\": \"date\"}, inplace=True) # Rename columns\n",
    "                    df['date'] = pd.to_datetime(df['date']) # Change date datatype to datetime\n",
    "                    df['precipitation'] = df['precipitation'].str.replace(',', '.') # Change punctuation to international\n",
    "                    df['precipitation'] = df['precipitation'].astype(float) # Change datatype to float\n",
    "                # ------------------------------------------------------------------------------------------------\n",
    "                if file[2]=='QRIO1' or file[2]=='QRIO2':\n",
    "                    df.drop(['Fecha máximo','Fecha mínimo'], axis = 1, inplace = True) # Drop redundant columns\n",
    "                    df.rename(columns={\"Máximo\": \"discharge_max\",\"Mínimo\": \"discharge_min\",\"Media\": \"discharge_mean\", \"fecha \": \"date\"}, inplace=True) # Rename columns\n",
    "                    df['date'] = pd.to_datetime(df['date']) # Change date datatype to datetime\n",
    "                    df['discharge_max'] = df['discharge_max'].str.replace(',', '.') # Change punctuation to international\n",
    "                    df['discharge_min'] = df['discharge_min'].str.replace(',', '.') # Change punctuation to international\n",
    "                    df['discharge_mean'] = df['discharge_mean'].str.replace(',', '.') # Change punctuation to international\n",
    "                    df['discharge_max'] = df['discharge_max'].astype(float) # Change datatype to float\n",
    "                    df['discharge_min'] = df['discharge_min'].astype(float) # Change datatype to float\n",
    "                    df['discharge_mean'] = df['discharge_mean'].astype(float) # Change datatype to float\n",
    "                    \n",
    "                    df.drop(['discharge_max','discharge_min'], axis = 1, inplace = True) # Drop redundant columns\n",
    "                # ------------------------------------------------------------------------------------------------\n",
    "                if file[2]=='HUMED':\n",
    "                    df.drop(['Fecha máximo','Fecha mínimo'], axis=1,inplace=True) # Drop redundant columns\n",
    "                    df.rename(columns={\"Máximo\": \"rel.humidity_max\",\"Mínimo\": \"rel.humidity_min\",\"Media\": \"rel.humidity_mean\", \"fecha \": \"date\"}, inplace=True) # Rename columns\n",
    "                    df['date'] = pd.to_datetime(df['date']) # Change date datatype to datetime\n",
    "                    if df['rel.humidity_min'].dtypes != 'int64':\n",
    "                        df['rel.humidity_min'] = df['rel.humidity_min'].str.replace(',', '.') # Change punctuation to international\n",
    "                    df['rel.humidity_max'] = df['rel.humidity_max'].str.replace(',', '.') # Change punctuation to international\n",
    "                    df['rel.humidity_mean'] = df['rel.humidity_mean'].str.replace(',', '.') # Change punctuation to international\n",
    "                    df['rel.humidity_max'] = df['rel.humidity_max'].astype(float) # Change datatype to float\n",
    "                    df['rel.humidity_min'] = df['rel.humidity_min'].astype(float) # Change datatype to float\n",
    "                    df['rel.humidity_mean'] = df['rel.humidity_mean'].astype(float) # Change datatype to float\n",
    "                    \n",
    "                    df.drop(['rel.humidity_max','rel.humidity_min'], axis = 1, inplace = True) # Drop redundant columns\n",
    "                # ------------------------------------------------------------------------------------------------\n",
    "                if file[2]=='RADIA':\n",
    "                    df.drop(['Fecha máximo'], axis=1,inplace=True) # Drop redundant columns\n",
    "                    df.rename(columns={\"Máximo\": \"radiation_max\",\"Media\": \"radiation_mean\", \"fecha\": \"date\"}, inplace=True) # Rename columns\n",
    "                    df['date'] = pd.to_datetime(df['date']) # Change date datatype to datetime\n",
    "                    df['radiation_max'] = df['radiation_max'].str.replace(',', '.') # Change punctuation to international\n",
    "                    df['radiation_mean'] = df['radiation_mean'].str.replace(',', '.') # Change punctuation to international\n",
    "                    df['radiation_max'] = df['radiation_max'].astype(float) # Change datatype to float\n",
    "                    df['radiation_mean'] = df['radiation_mean'].astype(float) # Change datatype to float\n",
    "                    \n",
    "                    df.drop(['radiation_max'], axis = 1, inplace = True) # Drop redundant columns\n",
    "                # ------------------------------------------------------------------------------------------------\n",
    "                if file[2]=='VVIEN':\n",
    "                    # df.rename(columns={\"Serie de tiempo\": \"date\",\"Valor [m/s]\": \"windspeed\"}, inplace=True)\n",
    "                    df.rename(columns={\"FECHA\": \"date\",\"VALOR\": \"windspeed\"}, inplace=True)\n",
    "                    df['windspeed'] = df['windspeed'].str.replace(',', '.') # Change punctuation to international\n",
    "                    df.replace('---','np.nan', inplace=True)\n",
    "                    df['windspeed'] = df['windspeed'].astype(float) # Change datatype to float\n",
    "                    # df = df[df['date']>='03/10/2021'] # Filter for dates later then 02.10.2021\n",
    "                    df['date'] = pd.to_datetime(df['date']) # Change date datatype to datetime\n",
    "                    df = df.resample('D', on='date').mean() # Daily aggregation \n",
    "                    df.replace('np.nan','-9999.0', inplace=True)\n",
    "                    df.reset_index(inplace=True) # Reset index so the date isn't the index\n",
    "                # ------------------------------------------------------------------------------------------------\n",
    "                if file[2]=='INSOL':\n",
    "                    df.drop('Fecha acumulado', axis = 1, inplace=True)\n",
    "                    df.rename(columns={\"fecha \": \"date\",\"Acumulado\": \"sun_duration\"}, inplace=True)\n",
    "                    # df['sun_duration'] = df['sun_duration'].str.replace(',', '.') NOT NEEDED cause of integer\n",
    "                    df['date'] = pd.to_datetime(df['date'])\n",
    "                    df.replace('---','-9999.0', inplace=True)\n",
    "                    df.replace('np.nan','-9999.0', inplace=True)\n",
    "                    df['sun_duration'] = df['sun_duration'].astype(float)\n",
    "                    df.loc[df['sun_duration'] > 0, 'sun_duration'] = df['sun_duration']/3600\n",
    "                # ------------------------------------------------------------------------------------------------\n",
    "                parameter_array.append(file[2]) # Append parameter to parameter_array\n",
    "                df_main = pd.merge(df_main, df, on='date', how='left') # Merge the dataframe to the mainframe\n",
    "            else: continue # If not equal, ignore the station\n",
    "        # ------------------------------------------------------------------------------------------------\n",
    "        df_main.fillna(-9999, inplace=True) # Fill NaN-Values with -9999,0\n",
    "        # Save the mainframe as csv-file\n",
    "        df_main.to_csv(r'{}\\{}_{}.csv'.format(path,station,'_'.join(parameter_array)), index=False, sep='\\t')  \n",
    "    \n",
    "    return df_main\n",
    "#==================================================================================================================\n",
    "# Function to delete a specific row in multiple csv files. Specify the filepath and the row you want to delete.\n",
    "def delete_row(path, row_number):\n",
    "    os.chdir(path) # changing active path\n",
    "    file_list = glob.glob('*.{}'.format('csv')) # List all csv files in the specified path\n",
    "    for file in file_list: # Iterate through all the listed files\n",
    "        df = pd.read_csv(file, encoding = 'latin', sep = '\\t') # Read the file as dataframe\n",
    "        df.drop(row_number, axis = 0, inplace = True) # In case of the April-data this is the 2021-12-31 date row, delete it\n",
    "        df.to_csv(file, index = False, sep = '\\t') # Resave the file\n",
    "#==================================================================================================================\n",
    "# Function to create .csv Files for SAIHebro parameterwise\n",
    "# Necessary for the export to run JAMS\n",
    "# Also adds the required stationinfo, you must provide them as a txtfile with pathcoord\n",
    "    # #date A061 A062 ..\n",
    "    # #date A061 A062 ..\n",
    "    # elevation 213 213 ..\n",
    "    # x 100 100 ..\n",
    "    # y 123 123 ..\n",
    "# Requirments:\n",
    "    # Stationswise timeseries data, exported in function combine\n",
    "    # stationinfo (ID, elevation, x,y (in UTM epsg 32630 for aragon))\n",
    "def parameter_dat(path,start_date,end_date,pathcoord,outpath):\n",
    "    os.chdir(path) # changing active path\n",
    "    file_list = glob.glob('*.{}'.format('csv'))\n",
    "    times = pd.date_range(start=start_date,end=end_date).to_list()\n",
    "    # create empty dfs to store all the stations\n",
    "    df_dis = pd.DataFrame(data=times,columns=['#date'])\n",
    "    df_precip= pd.DataFrame(data=times,columns=['#date'])\n",
    "    df_Tmax= pd.DataFrame(data=times,columns=['#date'])\n",
    "    df_Tmin= pd.DataFrame(data=times,columns=['#date'])\n",
    "    df_Tmean= pd.DataFrame(data=times,columns=['#date'])\n",
    "    df_rhum= pd.DataFrame(data=times,columns=['#date'])\n",
    "    df_wind= pd.DataFrame(data=times,columns=['#date'])\n",
    "    df_sun= pd.DataFrame(data=times,columns=['#date'])\n",
    "    # iterate through the stationwise files\n",
    "    for file in file_list:\n",
    "        parameters = file.split('_')[1:-2] # extract the parameters messured on this station\n",
    "        station = file.split('_')[0] # extract the station\n",
    "        df = pd.read_csv(file, encoding = 'latin', sep = '\\t') # open the file\n",
    "        df['#date'] = pd.to_datetime(df['#date']) # change #date to datetime format\n",
    "        for parameter in parameters: #iterate through the meassured parameters\n",
    "            # discharge\n",
    "            if parameter == 'QRIO1':\n",
    "                df_x = df[['#date','discharge_mean']].copy() # create new frame with the needed data\n",
    "                df_x.rename(columns={\"discharge_mean\": \"{}\".format(station)}, inplace=True) # rename the parameter to stationID\n",
    "                df_dis = pd.merge(df_dis, df_x , on='#date', how='left') # store the station in the main parameterframe\n",
    "            # precipitation\n",
    "            if parameter == 'PACUM':\n",
    "                df_x = df[['#date','precipitation']].copy()\n",
    "                df_x.rename(columns={\"precipitation\": \"{}\".format(station)}, inplace=True)\n",
    "                df_precip = pd.merge(df_precip, df_x , on='#date', how='left')\n",
    "            # temperature\n",
    "            if parameter == 'TEMPE':\n",
    "                # split in min, max, mean\n",
    "                df_x = df[['#date','T_max']].copy()\n",
    "                df_x.rename(columns={\"T_max\": \"{}\".format(station)}, inplace=True)\n",
    "                df_Tmax = pd.merge(df_Tmax, df_x , on='#date', how='left')\n",
    "\n",
    "                df_x = df[['#date','T_min']].copy()\n",
    "                df_x.rename(columns={\"T_min\": \"{}\".format(station)}, inplace=True)\n",
    "                df_Tmin = pd.merge(df_Tmin, df_x , on='#date', how='left')\n",
    "\n",
    "                df_x = df[['#date','T_mean']].copy()\n",
    "                df_x.rename(columns={\"T_mean\": \"{}\".format(station)}, inplace=True)\n",
    "                df_Tmean = pd.merge(df_Tmean, df_x , on='#date', how='left')\n",
    "            # humidity\n",
    "            if parameter == 'HUMED':\n",
    "                df_x = df[['#date','rel.humidity_mean']].copy()\n",
    "                df_x.rename(columns={\"rel.humidity_mean\": \"{}\".format(station)}, inplace=True)\n",
    "                df_rhum = pd.merge(df_rhum, df_x , on='#date', how='left')\n",
    "            # windspeed\n",
    "            if parameter == 'VVIEN':\n",
    "                df_x = df[['#date','windspeed']].copy()\n",
    "                df_x.rename(columns={\"windspeed\": \"{}\".format(station)}, inplace=True)\n",
    "                df_wind = pd.merge(df_wind, df_x , on='#date', how='left')\n",
    "            # insolation\n",
    "            if parameter == 'INSOL':\n",
    "                df_x = df[['#date','sun_duration']].copy()\n",
    "                df_x.rename(columns={\"sun_duration\": \"{}\".format(station)}, inplace=True)\n",
    "                df_sun = pd.merge(df_sun, df_x , on='#date', how='left')\n",
    "            # skip this extras\n",
    "            if parameter == 'NEMBA':\n",
    "                continue\n",
    "            if parameter == 'VEMBA':\n",
    "                continue\n",
    "            if parameter == 'QCAU1':\n",
    "                continue\n",
    "    # assign dataframes name (merge resets the name) like the JAMS input .dat files\n",
    "    df_dis.name='orun'\n",
    "    df_precip.name='rain'\n",
    "    df_Tmax.name='tmax'\n",
    "    df_Tmin.name='tmin'\n",
    "    df_Tmean.name='tmean'\n",
    "    df_rhum.name='rhum'\n",
    "    df_wind.name='wind'\n",
    "    df_sun.name='sunh'\n",
    "    # store all frames in a list (easier to handle otherwise subfunctions would be needed)\n",
    "    frames = [df_dis,df_precip,df_Tmax,df_Tmin,df_Tmean,df_rhum,df_wind,df_sun]\n",
    "\n",
    "    # iterate through the dataframes\n",
    "    for frame in frames:\n",
    "        # read the stationinfos from txt file\n",
    "        dfcoord = pd.read_csv(pathcoord, encoding = 'latin', sep = '\\t')\n",
    "        # correct the nan values to -9999 for JAMS\n",
    "        frame.replace(np.nan,-9999.0,inplace=True)\n",
    "        # dfcoord should contain all the stations in the studyarea BUT not all of them will record all the params, \n",
    "        # detect the missing ones and drop them\n",
    "        not_matching = list(set(dfcoord.columns).difference(frame.columns))\n",
    "        dfcoord.drop(not_matching , axis = 1 , inplace = True)    \n",
    "        dfcoord.replace(\"#date\",\"ID\",inplace=True) # rename #date to ID (required by JAMS)\n",
    "        # assign a new dictionary with 1-2 keys for the dfcoord and the frame\n",
    "        frames_coord = {\"2\": frame, \"1\": dfcoord}\n",
    "        # merge dfcoord and frame. keys needed so you can adjust the order\n",
    "        result = pd.concat(frames_coord, keys=[\"1\", \"2\"])\n",
    "        # GIMMICK TIME\n",
    "        # Jams requires a row with: name stat1 stat2 stat3... for each station\n",
    "        # first fill a list with accordingly n numbers\n",
    "        number = list(range(0, len(frame.columns)))\n",
    "        for i in range(len(number)):\n",
    "            # add the string \"stat\" infront of the number\n",
    "            number[i] = 'stat'+ str(number[i])\n",
    "        # assign the string \"name\" as the first element in the list\n",
    "        number[0]='name'\n",
    "        # rename the column headers by resulting list\n",
    "        result.set_axis(number, axis=1, inplace=True)\n",
    "        # Save the csv\n",
    "        result.to_csv(r'{}\\{}.csv'.format(outpath,frame.name), index = False, sep = '\\t')\n",
    "#==================================================================================================================\n",
    "# Function to rename multiple files. Specific for downloaded SAIHEbro files.\n",
    "# Example:\n",
    "# DatosHistoricos_218844_EM25T05VVIEN.csv     to     EM25T05VVIEN.csv\n",
    "def rename_csv(filelist):\n",
    "    for file in filelist: # Iterate through the files\n",
    "        if len(file) == 39: # If the length of the filename is 39:\n",
    "            new = file.split('_')[-1] # Split the filenames by '_' and keep the last part of it\n",
    "            os.rename(file, new) # Rename the file rename(old name, new name)\n",
    "    print('Files renamed...')\n",
    "#==================================================================================================================\n",
    "# Function to combine 2 .csv Files into 1 large .csv\n",
    "# Requirements:\n",
    "    # Identical filenames with different endings in one folder. EXAMPLE:\n",
    "        # ..\\Year_Combi\\0_raw\\A061_QRIO1_PACUM_tab.csv\n",
    "        # ..\\Year_Combi\\0_raw\\A061_QRIO1_PACUM_2022.csv\n",
    "    # Date column should be named #date (like RBIS is accepting the first row in the file) otherwise change the code or\n",
    "        # you can use function add_firstsign as a prep approach\n",
    "def year_combi(path):\n",
    "    os.chdir(path) # changing active path\n",
    "    file_list = glob.glob('*.{}'.format('csv')) # List all csv files in the specified path\n",
    "    for i in range(0,len(file_list)-1,2): # Iterate through each 2n element in range of the length of  filelist -1 \n",
    "        df_old = pd.read_csv(file_list[i+1], encoding = 'latin', sep = '\\t') # open the i+1 element (2/2 Element of the pair)\n",
    "        df_old.rename(columns={\"# date\": \"#date\"}, inplace=True) # Rename \n",
    "        df_old['#date'] = pd.to_datetime(df_old['#date'])\n",
    "\n",
    "        df_new = pd.read_csv(file_list[i], encoding = 'latin', sep = '\\t') # open the i element (1/2 Element of the pair)\n",
    "        df_new.rename(columns={\"# date\": \"#date\"}, inplace=True)\n",
    "        df_new['#date'] = pd.to_datetime(df_new['#date'])\n",
    "\n",
    "        frames = [df_old,df_new]\n",
    "        result = pd.concat(frames)\n",
    "        result.to_csv(r'{}\\done\\{}_combi.csv'.format(path,file_list[i+1].split('.')[0]), index=False, sep='\\t')\n",
    "#=================================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cca0f466",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files renamed...\n"
     ]
    }
   ],
   "source": [
    "# Main\n",
    "#==================================================================================================================\n",
    "# Insert your path to your text- and .csv files\n",
    "path  = r'E:\\1_Data_process\\4_timeseries\\SAIHebro\\april_data\\0_basedata'\n",
    "#start_date = '2021-12-31'\n",
    "#end_date = '2022-04-30'\n",
    "start_date = '1998-01-01'\n",
    "end_date = '2021-12-31'\n",
    "os.chdir(path) # Change directory \n",
    "txt_lists = glob.glob('*.{}'.format('txt')) # List all subfiles in the path\n",
    "for i in range(len(txt_lists)):\n",
    "    txt_lists[i] = path+'\\\\'+txt_lists[i] # Append the fullpath to txt_lists array\n",
    "# Call the array_stations function to extract all station numbers from one text-file and list all station numbers\n",
    "all_stations = array_stations(txt_lists[0])\n",
    "#==================================================================================================================\n",
    "time_series_list = glob.glob('*.{}'.format('csv'))\n",
    "rename_csv(time_series_list)\n",
    "# Append the fullpath to time_s]eries_list array, add the station number \n",
    "for i in range(len(time_series_list)):\n",
    "    time_series_list[i] = [path+'\\\\'+time_series_list[i],time_series_list[i][:4],time_series_list[i][7:-4]]\n",
    "#==================================================================================================================\n",
    "# Check if the stations of the listed csv-files are equal to the stations listed in the text-files\n",
    "all_parameter = [] # Array to append the matching stations\n",
    "#==================================================================================================================\n",
    "for i in time_series_list: # Iterate through the listed csv-files\n",
    "    if i[1] in all_stations: # Equal with text-file ?\n",
    "        all_parameter.append(i) # If equal append them to all_paramter\n",
    "#==================================================================================================================\n",
    "output_path = r'E:\\1_Data_process\\4_timeseries\\SAIHebro\\april_data\\1_done'\n",
    "df = combine(all_stations,all_parameter,output_path,start_date,end_date) # Call the combine function\n",
    "#==================================================================================================================\n",
    "add_firstsign(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03a8d7fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added # in first row...\n"
     ]
    }
   ],
   "source": [
    "output_path = r'E:\\1_Data_process\\4_timeseries\\SAIHebro\\EM_remake\\1998'\n",
    "add_firstsign(output_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
